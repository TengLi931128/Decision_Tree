{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teng/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:182: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The construct of Decision Tree is done!\n",
      "test_acc: 0.81\n",
      "train_acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Decision Tree \n",
    "Li Teng\n",
    "27.03.2020\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def get_true_label(arr):\n",
    "    #quality >= 6 as good(1),<6 as bad(-1)\n",
    "    arr = np.sign(arr-5.5)\n",
    "    return arr\n",
    "\n",
    "def calc_prob(Data):\n",
    "    #comput the positive & negative prob of the data\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for d in Data:\n",
    "        if d[11] == 1:\n",
    "            positive +=1\n",
    "        else:\n",
    "            negative +=1\n",
    "    positive_prob = positive/(positive+negative)\n",
    "    negative_prob = negative/(positive+negative)\n",
    "    return positive_prob, negative_prob\n",
    "\n",
    "def get_class(pos_prob,neg_prob):\n",
    "    if pos_prob>0.99:\n",
    "        C = 1\n",
    "    elif neg_prob>0.99:\n",
    "        C = -1\n",
    "    else:\n",
    "        C = None\n",
    "    return C\n",
    "\n",
    "def esti_class(pos_prob,neg_prob):\n",
    "    if pos_prob>neg_prob:\n",
    "        C = 1\n",
    "    else:\n",
    "        C = -1\n",
    "    return C\n",
    "\n",
    "def get_Entropy(Data):\n",
    "    #comput the entropy of the data\n",
    "    positive_prob, negative_prob = calc_prob(Data)\n",
    "    #print('pos:',positive_prob)\n",
    "    #print('neg:',negative_prob)\n",
    "    if positive_prob == 0:\n",
    "        #print('pure negative')\n",
    "        Entropy = 0\n",
    "    elif negative_prob == 0:\n",
    "        #print('pure positive')\n",
    "        Entropy = 0\n",
    "    else:\n",
    "        Entropy = -(positive_prob*math.log(positive_prob,2)+negative_prob*math.log(negative_prob,2))\n",
    "    return Entropy\n",
    "\n",
    "def find_Split_values(Data,Feature):\n",
    "    #find all split values of the given Feature\n",
    "    All_values = list(set(Data[:,Feature]))\n",
    "    All_values.sort()\n",
    "    All_Split_values = []\n",
    "    for index in range(len(All_values)-1):\n",
    "        All_Split_values.append((All_values[index]+All_values[index+1])/2)\n",
    "    return All_Split_values\n",
    "\n",
    "def split_data(Data,Feature,Split_value):\n",
    "    #split data into 2 subdatas\n",
    "    lenth = 0\n",
    "    for d in Data:\n",
    "        if d[Feature] < Split_value:\n",
    "            lenth+=1\n",
    "        else:\n",
    "            break            \n",
    "    SubData1 = Data[:lenth,:]\n",
    "    SubData2 = Data[lenth:,:]\n",
    "    return SubData1, SubData2\n",
    "    \n",
    "def find_best_Split_values(Data,Feature):\n",
    "    #find best split values of the given Feature from All_split_values\n",
    "    data = Data[Data[:,Feature].argsort()]\n",
    "    Best_Data1 = None\n",
    "    Best_Data2 = None\n",
    "    Best_Entropy = float('inf')\n",
    "    Best_Value = 0\n",
    "    Data_entities = Data.shape[0]\n",
    "    All_Split_values = find_Split_values(Data,Feature)\n",
    "    for value in All_Split_values:\n",
    "        #print(\"split value:\",value)\n",
    "        SubData1, SubData2 = split_data(data,Feature,value)\n",
    "        Entropy = (SubData1.shape[0]*get_Entropy(SubData1) + SubData2.shape[0]*get_Entropy(SubData2))/Data_entities\n",
    "        if Entropy < Best_Entropy:\n",
    "            Best_Entropy = Entropy\n",
    "            Best_Value = value\n",
    "            Best_Data1 = SubData1\n",
    "            Best_Data2 = SubData2\n",
    "    #print(\"Feature:\",Feature,\"Value\",Best_Value)\n",
    "    return Best_Data1, Best_Data2, Best_Value, Best_Entropy\n",
    "\n",
    "def get_best_feature(Data):\n",
    "    #find the best feature according to the Gain\n",
    "    Best_Feature = None\n",
    "    Best_Split_value = None\n",
    "    Best_Entropy = float('inf')\n",
    "    Data1 = None\n",
    "    Data2 = None\n",
    "    for F in range(10):\n",
    "        #print('Feature:',F)\n",
    "        D1,D2,Split_Value,Entropy = find_best_Split_values(Data,F)\n",
    "        if Entropy < Best_Entropy:\n",
    "            #print('replace')\n",
    "            Best_Feature = F\n",
    "            Best_Entropy = Entropy\n",
    "            Best_Split_value = Split_Value\n",
    "            Data1 = D1\n",
    "            Data2 = D2\n",
    "    return Best_Feature, Data1, Data2, Best_Split_value\n",
    "\n",
    "class Node:\n",
    "    def __init__(self,Leaf,split_value,Feature,lchild=None,rchild=None,C=None):\n",
    "        #self.Dataset = Dataset\n",
    "        self.Leaf = Leaf\n",
    "        self.Feature = Feature\n",
    "        self.split_value = split_value\n",
    "        self.C = C\n",
    "        self.lchild = lchild\n",
    "        self.rchild = rchild\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self,First_split_value,First_split_feature):\n",
    "        self.root = Node(False,First_split_value,First_split_feature)\n",
    "        self.queue = []\n",
    "        self.queue.append(self.root)\n",
    "    \n",
    "    def add(self,value,feature,C):\n",
    "        #print('add C:',C)\n",
    "        Tree_node = self.queue[0]\n",
    "        if C == 1 or C == -1:\n",
    "            #this node should be a leaf\n",
    "            #print('leaf')\n",
    "            New_node = Node(True,None,None,None,None,C)\n",
    "            if Tree_node.lchild == None:\n",
    "                Tree_node.lchild = New_node\n",
    "            else:\n",
    "                Tree_node.rchild = New_node\n",
    "                self.queue.pop(0)\n",
    "        else:\n",
    "            #this is not a leaf\n",
    "            New_node = Node(False,value,feature)\n",
    "            if Tree_node.lchild == None:\n",
    "                Tree_node.lchild = New_node\n",
    "                self.queue.append(New_node)\n",
    "            else:\n",
    "                Tree_node.rchild = New_node\n",
    "                self.queue.append(New_node)\n",
    "                self.queue.pop(0)\n",
    "                \n",
    "    def predict(self,root,Data):\n",
    "        if root.Leaf == True:\n",
    "            #this is leaf node\n",
    "            predict_label = root.C\n",
    "            return predict_label\n",
    "        else:\n",
    "            if Data[root.Feature] < root.split_value:\n",
    "                return self.predict(root.lchild,Data)\n",
    "            else:\n",
    "                return self.predict(root.rchild,Data)\n",
    "            \n",
    "    def depth_first_search(self,root):\n",
    "        if root == None:\n",
    "            return None\n",
    "        else:\n",
    "            print(root.Leaf,root.C)\n",
    "            self.depth_first_search(root.lchild)\n",
    "            self.depth_first_search(root.rchild)\n",
    "\n",
    "\n",
    "#Data preprocessing \n",
    "data = pd.read_csv('/home/teng/Github/Decision_Tree/winequality-red.csv')\n",
    "data = data.as_matrix(columns=None)\n",
    "np.random.shuffle(data)\n",
    "Training_Set, Test_Set = data[:1499,:], data[1499:,:]\n",
    "#Col 0-10 are Features and Col 11 is label.\n",
    "Training_Set[:,11] = get_true_label(Training_Set[:,11])\n",
    "Test_Set[:,11] = get_true_label(Test_Set[:,11])\n",
    "\n",
    "Root_Feature, Data1, Data2, Root_Split_value = get_best_feature(Training_Set)\n",
    "#print(Root_Feature, Root_Split_value)\n",
    "#use a queue to store the Data which is not 'pure'\n",
    "queue = []\n",
    "queue.append(Data1)\n",
    "queue.append(Data2)\n",
    "#construct the tree\n",
    "Dec_Tree = Tree(Root_Split_value,Root_Feature)\n",
    "while queue:\n",
    "    Data = queue[0]\n",
    "    positive_prob, negative_prob = calc_prob(Data)\n",
    "    if Data.shape[0]<2:\n",
    "        #this Dataset is too small\n",
    "        C = esti_class(positive_prob,negative_prob)\n",
    "    else:\n",
    "        C = get_class(positive_prob,negative_prob)\n",
    "    #print(' ')\n",
    "    #print('Data:',Data.shape,' C:',C)\n",
    "    #print('p_prob:',positive_prob,' n_prob:',negative_prob)\n",
    "    if C==1 or C==-1:\n",
    "        #pure pos or neg\n",
    "        Dec_Tree.add(None,None,C)\n",
    "        queue.pop(0)\n",
    "    else:\n",
    "        #not pure\n",
    "        Feature, D1, D2, Split_value = get_best_feature(Data)\n",
    "        queue.append(D1)\n",
    "        queue.append(D2)\n",
    "        Dec_Tree.add(Split_value,Feature,None)\n",
    "        queue.pop(0)\n",
    "print('The construct of Decision Tree is done!')\n",
    "\n",
    "correct_instence = 0\n",
    "wrong_instence = 0\n",
    "for ins in Test_Set:\n",
    "    predict_label = Dec_Tree.predict(Dec_Tree.root,ins)\n",
    "    True_label = ins[11]\n",
    "    if predict_label == True_label:\n",
    "        correct_instence += 1\n",
    "    else:\n",
    "        wrong_instence += 1\n",
    "acc = correct_instence/(correct_instence+wrong_instence)\n",
    "print('test_acc:',acc)\n",
    "\n",
    "correct_instence = 0\n",
    "wrong_instence = 0\n",
    "for ins in Training_Set:\n",
    "    predict_label = Dec_Tree.predict(Dec_Tree.root,ins)\n",
    "    True_label = ins[11]\n",
    "    #print(predict_label, True_label)\n",
    "    if predict_label == True_label:\n",
    "        correct_instence += 1\n",
    "    else:\n",
    "        wrong_instence += 1\n",
    "acc = correct_instence/(correct_instence+wrong_instence)\n",
    "print('train_acc:',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
